# 安装步骤
1. 准备环境
2. 安装JDK
3. 上传spark安装包
4. 解压spark并修改配置文件（两个配置文件，第一个配置文件添加了3个配置文件）
5. 将配置好的spark安装程序拷贝给其他机器for i in {5..8}; do scp -r /bigdata/spark-2.2.0-bin-hadoop2.7/ node-$i:/bigdata; done 

6. 启动spark (sbin/start-all.sh)  
  问题：Worker怎么知道Master在哪里嗯？读取spark-env.sh文件得知Master在哪里的
7. 通过web页面访问spark管理页面（master所在机器的地址+8080端口）

-----------------------------
# 配置高可用的spark集群
1. 修改配置文件spark-env.sh
```
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=节点:2181,节点:2181 -Dspark.deploy.zookeeper.dir=spark2/"
```
2. 配置Worker运行时的资源（内存、cores）
```
export SPARK_WORKER_MEMORY=20g
```

3.启动集群
	1.启动ZK集群
	2.启动spark集群，但是只会启动一个Master，另外一个Master手动启动





















